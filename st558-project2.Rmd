---
title: "Predicting Popularity of Articles in the `r channel` Channel"
author: "Claudia Donahue and Dane Korver"
date: '2022-06-28'
output: 
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = paste("./", channel, "_images/", sep = ""))
```

## Introduction    

For this project, the [dataset](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) we are using summarizes a heterogeneous set of statistics about articles published by Mashable over a period of two years. The goal is to predict an article's number of shares to social networks (its popularity). We wanted to look at the patterns in the articles that were shared. For example, is the timing of the article, the headline, and the article's content all relevant in determining the number of times the article gets shared? What about whether an article had a polarizing title versus a generic non-polarizing title. Then, we wanted to find out whether the number of keywords associated with an article impacted the number of shares it received.  Here are our findings for studying how to predict the number of shares in social networks (popularity).  

## Data  

We'll begin by reading in the data set and looking at how it's structured.  

```{r Read in data using a relative path}
data <- readr::read_csv(file = "OnlineNewsPopularity.csv",
                        show_col_types = FALSE
                        )
# Look at structure of data set
str(data)

# Checking to see whether the data has missing values
sum(is.na(data))
```

The data has just one column that is not numeric, and that column is the first one and contains URLs for the Mashable articles for each observation. We will keep it, but won't use it in our models. The second column, `timedelta`, is not useful for prediction either. We will drop this one. The other columns contain numeric data that we may be able to use to predict the number of shares. The last column is our target variable, `shares`. The data is set up nicely for what we want to do.  

```{r drop and clean up columns, message=FALSE}

# Dropping the timedelta column

library(plyr)
library(tidyverse)
data <- select(data, -timedelta)

# Add a day column for data exploration/plotting purposes
data$day <- case_when(
  data$weekday_is_monday == 1 ~ "Monday",
  data$weekday_is_tuesday == 1 ~ "Tuesday",
  data$weekday_is_wednesday == 1 ~ "Wednesday",
  data$weekday_is_thursday == 1 ~ "Thursday",
  data$weekday_is_friday == 1 ~ "Friday",
  data$weekday_is_saturday == 1 ~ "Saturday",
  data$weekday_is_sunday == 1 ~ "Sunday"
)
data$day <- as_factor(data$day)

#Converting categorical values from numeric to factor - Weekdays
data$weekday_is_monday <- factor(data$weekday_is_monday)
data$weekday_is_tuesday <- factor(data$weekday_is_tuesday)
data$weekday_is_wednesday <- factor(data$weekday_is_wednesday)
data$weekday_is_thursday <- factor(data$weekday_is_thursday)
data$weekday_is_friday <- factor(data$weekday_is_friday)
data$weekday_is_saturday <- factor(data$weekday_is_saturday)
data$weekday_is_sunday <- factor(data$weekday_is_sunday)

# Add a channel column 
data$chan <- case_when(
  data$data_channel_is_lifestyle == 1 ~ "Lifestyle",
  data$data_channel_is_entertainment == 1 ~ "Entertainment",
  data$data_channel_is_bus == 1 ~ "Business",
  data$data_channel_is_socmed == 1 ~ "Social Media",
  data$data_channel_is_tech == 1 ~ "Technology",
  data$data_channel_is_world == 1 ~ "World"
)
data$chan <- as_factor(data$chan)

#Converting categorical values from numeric to factor - News subjects
data$data_channel_is_lifestyle <- factor(data$data_channel_is_lifestyle) 
data$data_channel_is_entertainment <- factor(data$data_channel_is_entertainment)
data$data_channel_is_bus <- factor(data$data_channel_is_bus)
data$data_channel_is_socmed <- factor(data$data_channel_is_socmed)
data$data_channel_is_tech <- factor(data$data_channel_is_tech)
data$data_channel_is_world <- factor(data$data_channel_is_world)

```

Next we will begin our look at one specific channel (`r channel`) by subsetting the data.  

```{r Subset channel of interest, message=FALSE}
channel <- channel # set = to channel when ready to automate

channelNow <- paste("data_channel_is_", channel, sep = "")

cData <- data[data[channelNow] == 1, ] # Extract rows of interest

```

We then split the `r channel` channel's data into training and testing sets (70% and 30%, respectively). We will only explore the training set, and will keep the testing set in reserve to determine the quality of our predictions. We will use the function `createDataPartition()` from the `caret` package to split the data. 

```{r Split data into training and testing, message=FALSE}
library(caret) # Using createDataPartition from caret

set.seed(33) # for reproducibility 

# Index to split on
idx <- createDataPartition(y = cData$shares, p = 0.7, list = FALSE)

# Subset
training <- cData[idx, ]
testing <- cData[-idx, ]

```

## Summarizations  

Then we thought about the characteristics of an online article that might be associated with someone deciding to "share" the article to someone else. 

We thought it was probably important to consider both the timing of the article, the headline, and the article's content. By timing, we mean that perhaps some readers are more likely to click on an article and share it on the weekend because they generally have more free time to read. But then we plotted the number of articles published each day, and realized not much gets published on the weekend, compared to weekdays. To visualize this pattern, we created the chart below:  

```{r bar by day of week, message=FALSE}

ggplot(data) + 
  geom_bar(aes(x = day, fill = chan)) +
  labs(title = "Number of Articles by Day of Week",
       x = "Day of the Week",
       y = "Number of Articles",
       fill = "Channel")
```

So we did away with that theory, and we will instead look at just the `r channel` channel's number of shares across days of the week. 

```{r boxplot of shares by day of week}
ggplot(training, aes(x = day, y = shares)) +
  geom_boxplot() + 
  geom_jitter(aes(color = day)) + 
  ggtitle("Boxplot for Shares")
```


The boxplot shows the distribution of the number of shares by the day of the week. It can be a good way to see if we have any outliers with **way** more shares than a typical article in this channel.  

We wanted to look at these outliers--the `r channel` channel's top articles by shares, so we grabbed a list of those URLs, along with the number of shares. 

```{r Top articles}
head(training[order(training$shares, decreasing = TRUE), c("url", "shares")], 10)
```
You can check out the article's date and title within the URL and see what some of the most-shared articles were in the `r channel` channel during the time period studied.  

Then we wanted to create a visualization that would show us how the variable `title_sentiment_polarity` seemed to impact the number of shares. Our thought was that maybe an article with a more polarizing title would get more shares than one less polarizing, as people want to have some justification for urging a friend to spend time reading the article. A polarizing sentiment could provide that justification for some people. We will plot the polarity of the title's sentiment and include information on the number of words in the title.    

```{r Scatter Plot title impact on shares, warning=FALSE}
ggplot(training, aes(x = title_sentiment_polarity,
                     y = shares,
                     color = n_tokens_title)) +
  geom_point() +
  labs(title = "Title Sentiment vs Number of Shares",
       x = "Sentiment Polarity of Title",
       y = "Number of Shares",
       color = "# Words in Title")
```

In this plot of the impact of the title's sentiment polarity on shares, an upward trend in the plotted points would indicate that articles with higher values of title sentiment polarity tend to be shared more often. Note that polarity values can be positive or negative. 

Finally, we thought about how the content of an article might lead someone to share it. Maybe people share shorter articles more than long ones. Maybe people like to share links with images more than links without images, we thought. So we took a look at an article's length and number of images vs. number of shares.  


```{r Scatter plot of Article content length and images, warning=FALSE}
ggplot(training, aes(x = n_tokens_content,
                     y = shares,
                     color = num_imgs)) +
  geom_point() +
  labs(title = "Article Length vs Number of Shares",
       x = "Number of Words in Article",
       y = "Number of Shares",
       color = "# Images")

```

In this plot, a downward trend in plotted points would indicate that shorter articles generally get more shares, while an upward trend would indicate that longer articles achieve more shares. 

Next we looked at an article's keyword characteristics. Within its metadata, a website can be assigned a number of keywords, which used to give search engines more information about the content. We wondered how the number of keywords related to the number of shares, given that this data is several years old, and that used to be considered a part of search engine optimization.  

```{r Histogram of Keywords vs shares}
ggplot(training, aes(x = num_keywords,
                     y = shares)) +
  geom_count() +
  labs(title = "Number of Keywords vs. Shares",
       x = "Number of Keywords",
       y = "Number of Shares")
```

The plot depicts circles sized by the number of articles falling into that category of number of keywords and number of shares. So you can how many keywords are typically used, and also whether any specific number of keywords tends to achieve more shares.  

Before the modeling step, we took one final look at a few more of the other variables we thought might be important in predicting number of shares, based on the summaries above and our own experiences. 

```{r exploratory graph, message=FALSE}
library(GGally)
training %>% 
  select(self_reference_avg_sharess, LDA_00, rate_negative_words, shares) %>%
  GGally::ggpairs()
```

Looking across the bottom row of graphs, we can see whether any relationships between `shares` and another variable are evident. 

## Modeling  

Now we were ready to create some predictive models using the training data.  

### Linear Regression  
Linear regression is a way of calculating the relationship between one or more input/independent variables and an output/dependent variable. (More than one input variable would make the model a multiple regression) In this case, our output variables is `shares`, the number of times a Mashable article was shared. A linear regression assumes that one or more other variables are correlated with the number of shares, and that the relationship can be visually represented as a straight line. The mathematical equation for a basic linear regression is: 

$y = B*x+A$

where y is the dependent variable, x is an independent variable, and A and B are coefficients for the line's y-intercept and slope. The values for these coefficients are chosen to minimize the error between the model's predictions and the actual outcomes in the training data. 

With that, here we go! We are using the `train()` function from the `caret` package to make the model. We will use the results of 5-fold cross-validation to evaluate the performance of all our models and, later, to compare them.

This first multiple regression (linear regression model extended to include more explanatory variables and/or higher order terms) will try to predict the number of shares based on most of the available data in our training dataset.  

```{r multiple regression full fit, warning=FALSE}
# building the model
fullFit <- train(shares ~ n_tokens_content + num_hrefs + num_self_hrefs +
                     average_token_length + num_keywords + 
                     kw_min_max + kw_max_max + kw_avg_max + kw_max_avg +
                     kw_avg_avg + self_reference_min_shares + weekday_is_monday +
                     weekday_is_tuesday + weekday_is_wednesday + 
                     weekday_is_thursday + weekday_is_friday +
                     global_subjectivity + title_sentiment_polarity, 
              data = training, 
              method = "lm", # linear regression
              preProcess = c("center", "scale", "nzv"),
              trControl = trainControl(method = "cv", number = 2)
              )
# look at the resulting coefficients
fullFit$finalModel
fullFit
```

The next multiple regresson model is a little more simplified and includes a smaller subset of variables that we think would be important. 

```{r multiple regression w/selected variables, warning=FALSE}
smallFit <- train(shares ~ n_tokens_content + num_hrefs + num_self_hrefs +
                     average_token_length + num_keywords + 
                     kw_min_max + kw_max_max + kw_avg_max + kw_max_avg +
                     kw_avg_avg + self_reference_min_shares +
                     global_subjectivity + title_sentiment_polarity,
              data = training, 
              method = "lm", 
              preProcess = c("center", "scale", "nzv"),
              trControl = trainControl(method = "cv", number = 2)
              )
# look at the resulting coefficients
smallFit$finalModel
smallFit
```


### Random Forest  

Random forest is a tree-based method of prediction. It does not use all predictors available, but instead uses a random subset of predictors for each of many bootstrap samples / tree fits.  

```{r Random Forest model, message = FALSE, warning = FALSE}
# load required package
library(randomForest)
# set up the mtry parameter 
tunegrid <- expand.grid(.mtry=c(1:3)) # This is key for amount of time running

#train model
rfFit <- train(x = select(training, -url, -shares, -day, -chan), 
                y = training$shares,
                method = "rf",
                tuneGrid = tunegrid,
                preProcess = c("center", "scale", "nzv"),
                trControl = trainControl(method = "cv", 
                                         number = 2)
                )
rfFit

```

### Boosted Tree  

```{r boosted tree model, message=FALSE, warning=FALSE}
# Load required packages
library(gbm)

# set up the parameters
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3), 
                        n.trees = c(25, 50), 
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

#train model
btFit <- train(x = select(training, -url, -shares, -day, -chan), 
                y = training$shares,
                method = "gbm",
                tuneGrid = gbmGrid,
                preProcess = c("center", "scale", "nzv"),
                trControl = trainControl(method = "cv", 
                                         number = 2)
                )

```


## Comparison  

We compared all four of these models using the test dataset.  

```{r test set}
# full fit multiple regression model
fullPred <- predict(fullFit, newdata = testing)
# selected variable multiple regression model
smallPred <- predict(smallFit, newdata = testing)
# random forest
rfPred <- predict(rfFit, newdata = testing)
# boosted tree
btPred <- predict(btFit, newdata = testing)
```


Now we will compare the four candidate models and choose one "winner":  

```{r compare models and choose}
# Create a named with results (Rsquared values) for each model
results <- c("Full Fit" = max(fullFit$results$Rsquared), 
           "Small Fit" = max(smallFit$results$Rsquared),
           "Random Forest" = max(rfFit$results$Rsquared),
           "Boosted Tree" = max(btFit$results$Rsquared))

# RSquared Values are: 
results

# The best model based on the highest R-squared value is:
winner <- results[which.max(results)]
winner
``` 

Above is our winning model for the `r channel` channel based on it having the highest R-Squared value of `r winner`! Our models are not doing that great, and only explain a small percentage of the variation in number of shares, but let's not let that dampen our enthusiasm!   

If we wanted to improve upon these models, we could increase the tuning value of `mtry` in the Random Forest model. The cost would be the model would take a lot longer to train. We also considered trying to predict the  $log(`shares`)$  instead of `shares` itself, but decided that was outside the scope of the assignment. 

## Automation  

We generated these reports automatically for each channel ("lifestyle", "entertainment", "bus", "socmed", "tech", and "world") by creating a function that uses the `rmarkdown` package to render a Github document with a `params` option, and then using a for loop to execute that function for each channel in a list. That's how the `r channel` channel page you're reading was generated!  

The code we used to automate the rendering is visible at the main page for this project [here](https://cdonahu.github.io/st558-project2/). 






